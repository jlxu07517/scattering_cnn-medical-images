{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage.restoration import (denoise_wavelet)\n",
    "from skimage import exposure\n",
    "from torch.utils import data\n",
    "import pickle\n",
    "from torchvision import transforms\n",
    "from kymatio import Scattering2D\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.2\n"
     ]
    }
   ],
   "source": [
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cancer(data.Dataset):\n",
    "    def __init__(self, parent_dir,samplerate,transform=None,phase = 'trainc',load_prob = True,green = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            transform (optional): Optional transform to be applied on a sample.\n",
    "            green: If true, only take the green channel\n",
    "            samplerate: take how much percent of the original tadaset\n",
    "            Note: Whenever you change a samplerate, you need to resave the pickle\n",
    "        \"\"\"\n",
    "        self.green = green\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "        if not parent_dir.endswith('/'):\n",
    "            # Make sure the directory name is correctly given\n",
    "            parent_dir = parent_dir + '/'\n",
    "\n",
    "        data_list = []\n",
    "        for lab in ['/malignant/','/benign/']:\n",
    "            filelist  = glob.glob(parent_dir +lab+ '**/40X/*.png', recursive=True)\n",
    "            data_list.extend([(file,0) if lab == '/benign/' else (file,1) for file in filelist]) #include labels before random split\n",
    "        \n",
    "        self.data_list = data_list #All the (filename,label)\n",
    "        \n",
    "#         self.num_allsamples = len(data_list)\n",
    "#         self.transform = transform\n",
    "    \n",
    "        random.seed(3)\n",
    "        random.shuffle(data_list)       \n",
    "        trainlst = data_list[:round(samplerate*len(data_list))]\n",
    "        testlst = data_list[round(samplerate*len(data_list)):]\n",
    "       \n",
    "        \n",
    "        if phase == 'trainc':\n",
    "            jpg_list = trainlst\n",
    "        else:\n",
    "            jpg_list = testlst\n",
    "      \n",
    "        \n",
    "        self.image_data_dict = {}\n",
    "\n",
    "        \n",
    "        if load_prob:\n",
    "            f_myfile = open(phase + '.pickle', 'rb')\n",
    "            self.image_data_dict = pickle.load(f_myfile)\n",
    "            f_myfile.close()\n",
    "        else:\n",
    "            for i in range(len(jpg_list)):\n",
    "                #dataset = np.array(Image.open(jpg_list[i][0]).convert('LA'))\n",
    "                dataset = imageio.imread(jpg_list[i][0]) #rgb\n",
    "                self.image_data_dict[i] = [dataset, jpg_list[i][1]] #pixle and label\n",
    "            with open(phase + '.pickle', 'wb') as handle:\n",
    "                pickle.dump(self.image_data_dict, handle)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data_dict)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        '''\n",
    "        Return a tuple containing the image tensor and corresponding class for the given index.\n",
    "        Parameter:\n",
    "        index: This is the index created by _init_, it's the key of the dict in _init_\n",
    "               Notice that a single patient could have multiple index associated.\n",
    "        '''\n",
    "        if index not in self.image_data_dict:\n",
    "            raise ValueError('Index out of bound')\n",
    "        img,tag = self.image_data_dict[index]\n",
    "        \n",
    "       #isolating green channel:\n",
    "        if self.green:\n",
    "            img = img[:,:,1]\n",
    "            \n",
    "        img = transforms.ToPILImage()(img)\n",
    "        img = transforms.functional.resize(img,(60,60))\n",
    "          \n",
    "       \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        sample = (img, tag)\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(epoch,model):\n",
    "    torch.save(model.state_dict(), \"Cancer_CNN_trial_{}.model\".format(epoch))\n",
    "    print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size = 3, stride = 1, padding = 0, groups=1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, momentum = 0.1)\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(16, 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv2d(16, 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "#             nn.ReLU(inplace=True))\n",
    "#         self.batchnorm2 = nn.BatchNorm2d(32, momentum = 0.1)\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc_layer = nn.Sequential(nn.Linear(13456, 2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.layer1(x)\n",
    "        output = self.batchnorm1(output)\n",
    "        #output = self.layer2(output)\n",
    "        #output = self.layer3(output)\n",
    "        #output = self.batchnorm2(output)\n",
    "        output = output.view(output.size(0), -1) # flatten\n",
    "        output = self.fc_layer(output)\n",
    "        return output#nn.functional.softmax(output, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/ipykernel_launcher.py:84: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.847889537494387\n",
      "recall = 0.8549618320610687\n",
      "accuracy = 0.8040201005025126\n",
      "accuracy from bpreds = 0.8040201005025126\n",
      "Epoch 0: train_accuracy is 0.7383073496659243, test_accuracy is 0.8040201005025126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-9:\n",
      "Process Process-10:\n",
      "Process Process-11:\n",
      "Process Process-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-2-8f48a4e391aa>\", line 78, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 163, in __call__\n",
      "    return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 208, in normalize\n",
      "    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"<ipython-input-2-8f48a4e391aa>\", line 74, in __getitem__\n",
      "    img = transforms.functional.resize(img,(60,60))\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 246, in resize\n",
      "    return img.resize(size[::-1], interpolation)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/jx1047/.conda/envs/image/lib/python3.6/site-packages/PIL/Image.py\", line 1712, in resize\n",
      "    return self._new(self.im.resize(size, resample))\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-645a2904dbac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m#a = list(train_model.parameters())[0].clone()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m#b = list(train_model.parameters())[0].clone()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/image/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/image/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#trainmean 0.8058, 0.6531, 0.7753 trainstd0.08847197, 0.13046794, 0.0852273 testmean0.8009, 0.6541, 0.7711 teststd 0.11159356, 0.12982282, 0.10046051\n",
    "allauc = {}\n",
    "allrecall = {}\n",
    "allacc = {}\n",
    "pdir = '/scratch/jx1047/project/Scattering-colonography/breast'\n",
    "for rate in [0.9]:\n",
    "    #Update the new file\n",
    "    img = cancer(pdir,samplerate = rate,phase =  'trainc',load_prob = False)\n",
    "    img = cancer(pdir,samplerate = rate,phase =  'testc',load_prob = False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    trainimg = cancer(pdir,samplerate = 1,phase = 'trainc',load_prob = True,transform = transforms.Compose([transforms.ToTensor(),\n",
    "            transforms.Normalize((0.8058, 0.6531, 0.7753),(0.08847197, 0.13046794, 0.0852273 ))]))\n",
    "    testimg = cancer(pdir,samplerate = 1,phase = 'testc',load_prob = True,transform = transforms.Compose([transforms.ToTensor(),\n",
    "             transforms.Normalize(( 0.8009, 0.6541, 0.7711),(0.11159356, 0.12982282, 0.1004605,))]))\n",
    "    \n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.cuda.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    params = {'batch_size': 32, \n",
    "              'shuffle': True,\n",
    "              'num_workers': 4}\n",
    "    patch_training_generator = data.DataLoader(trainimg, **params)\n",
    "    patch_test_generator = data.DataLoader(testimg,**params)\n",
    "    \n",
    "    train_model = ConvNet()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(train_model.parameters(), lr=0.01)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5)\n",
    "\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_acc1 = 0.0\n",
    "    num_epochs = 15\n",
    "    predall = [] #max\n",
    "    predprob = [] #\n",
    "    truelabel = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_model.train()\n",
    "        train_acc = 0.0\n",
    "        lentra = 0\n",
    "        lente = 0\n",
    "        for i, (images, label) in enumerate(patch_training_generator):\n",
    "            lentra += images.size()[0]\n",
    "            #images.requires_grad_(True)\n",
    "            images = Variable(images, requires_grad=True)\n",
    "            label = Variable(label)\n",
    "            # Run the forward pass\n",
    "            outputs = train_model(images)\n",
    "            loss = criterion(outputs, label)\n",
    "            #a = list(train_model.parameters())[0].clone()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #b = list(train_model.parameters())[0].clone()\n",
    "            #print(torch.equal(a.data, b.data))\n",
    "            # Store result\n",
    "            #print(outputs.data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Save train accuracy\n",
    "            train_acc += torch.sum(predicted == label.data)\n",
    "\n",
    "        train_acc = float(train_acc) / lentra\n",
    "        scheduler.step()\n",
    "        # Check test accuracy, if the accuracy is higher than before, save the model\n",
    "\n",
    "        train_model.eval()\n",
    "        for child in train_model.children():\n",
    "            if type(child)==nn.BatchNorm2d:\n",
    "                child.track_running_stats = False\n",
    "        ta = 0.0\n",
    "        predall = [] #max\n",
    "        predprob = [] #\n",
    "        truelabel = []\n",
    "\n",
    "        for i, (image, label) in enumerate(patch_test_generator):\n",
    "            lente += image.size()[0]\n",
    "            images = Variable(image)\n",
    "            label = Variable(label)\n",
    "            out = train_model(images)\n",
    "            pos_proba = torch.nn.functional.softmax(out.data)[:,1]\n",
    "            predprob.append(pos_proba)\n",
    "            truelabel.append(label.data)  \n",
    "            _, prediction = torch.max(out.data,1)  \n",
    "            prediction2 = (pos_proba > 0.5)*1 #prediction based on 0.5 threshold\n",
    "            predall.append(prediction)\n",
    "            prediction2 = (pos_proba > 0.5)*1\n",
    "            ta1 =+ np.sum(prediction2.numpy() == label.data.numpy()) #used to calculate the accuracy based on 0.5 threshold\n",
    "            ta += torch.sum(prediction == label.data)\n",
    "        labels = torch.cat(truelabel).numpy()\n",
    "        preds = torch.cat(predprob).numpy()\n",
    "        bpreds = torch.cat(predall).numpy()\n",
    "        ta = float(ta) / lente #test size\n",
    "        auc = roc_auc_score(labels, preds)\n",
    "        recall = np.sum((bpreds == labels )&(labels == 1))/np.sum(labels == 1)\n",
    "        print('auc = {}'.format(auc))\n",
    "        print('recall = {}'.format(recall))\n",
    "        print('accuracy = {}'.format(ta))\n",
    "        print('accuracy from bpreds = {}'.format(np.sum(bpreds == labels)/len(labels)))\n",
    "        print(\"Epoch {}: train_accuracy is {}, test_accuracy is {}\".format(epoch, train_acc, ta))\n",
    "        if epoch >= 10:\n",
    "            if rate not in allauc.keys():\n",
    "                allauc[rate] = [auc]\n",
    "                allrecall[rate] = [recall]\n",
    "                allacc[rate] = [ta]\n",
    "            else:\n",
    "                allauc[rate].append(auc)\n",
    "                allrecall[rate].append(recall)\n",
    "                allacc[rate].append(ta)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 60, 60])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avauc = {key: np.mean(val) for key,val in allauc.items()}\n",
    "avrecall = {key: np.mean(val) for key,val in allrecall.items()}\n",
    "avacc = {key: np.mean(val) for key,val in allacc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.9: [0.87876066457117208,\n",
       "  0.88358778625954193,\n",
       "  0.88078132016165245,\n",
       "  0.88224068253255505,\n",
       "  0.88370004490345755]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.9: [0.8241206030150754,\n",
       "  0.8140703517587939,\n",
       "  0.8140703517587939,\n",
       "  0.8090452261306532,\n",
       "  0.8190954773869347]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.9: [0.8854961832061069,\n",
       "  0.87786259541984735,\n",
       "  0.87786259541984735,\n",
       "  0.87786259541984735,\n",
       "  0.89312977099236646]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allrecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
